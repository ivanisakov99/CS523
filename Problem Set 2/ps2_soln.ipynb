{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecSh-LQTDB7H"
   },
   "source": [
    "# Problem Set 2 \n",
    "*Author: Saenko*\n",
    "\n",
    "---\n",
    "\n",
    "This assignment will introduce you to:\n",
    "1. Basic functionality in PyTorch\n",
    "2. Building and training a convolutional network\n",
    "3. Visualizations using Tensorboard (optional)\n",
    "\n",
    "This code has been tested on Colab. You may want to run parts of it on a GPU.\n",
    "**Warning:** If you use the SCC, the gpu queue may be long when the deadline comes. Please start your homework early.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Problem 0: Tutorials\n",
    "This homework will introduce you to [PyTorch](https://pytorch.org), currently the fastest growing deep learning library.\n",
    "\n",
    "Before starting the homework, please go over these introductory tutorials on the PyTorch webpage:\n",
    "\n",
    "*   [60-minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    " \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CcqKCRlM6poM"
   },
   "source": [
    "# Problem 1: Torch Intro (30 points)\n",
    "The `torch.Tensor` class is the basic building block in PyTorch and is used to hold data and parameters. The `autograd` package provides automatic differentiation for all operations on Tensors. After reading about Autograd in the tutorials above,  we will implement a few simple examples of what Autograd can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUv0o0dIsLlW"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D15InPOpsLlc"
   },
   "source": [
    "### 1.1. Simple function\n",
    " Use `autograd` to do backpropagation on the simple function we saw in lecture, $f=(x+y)*z$. \n",
    "\n",
    "**1.1.1** Create the three inputs with values $x=-2$, $y=5$ and $z=-4$ as tensors and set `requires_grad=True` to track computation on them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1581997995146,
     "user": {
      "displayName": "Samarth Mishra",
      "photoUrl": "",
      "userId": "00064519813488481618"
     },
     "user_tz": 300
    },
    "id": "56mcFemZuYMB",
    "outputId": "4be4f837-cc1b-4579-d97b-edad8dd68263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.], requires_grad=True) tensor([5.], requires_grad=True) tensor([-4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "x = torch.tensor([float(-2)], requires_grad=True)\n",
    "y = torch.tensor([float(5)], requires_grad=True)\n",
    "z = torch.tensor([float(-4)], requires_grad=True)\n",
    "print(x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPBbS7GWxra1"
   },
   "source": [
    "**1.1.2** Compute the $q=x+y$ and $f=q \\times z$ functions, creating tensors for them in the process. Print out $q,f$, then run `f.backward(retain_graph=True)`, to compute the gradients w.r.t. $x,y,z$. The `retain_graph` attribute tells autograd to keep the computation graph around after backward pass as opposed deleting it (freeing some memory). Print the gradients. Note that the gradient for $q$ will be `None` since it is an intermediate node, even though `requires_grad` for it is automatically set to `True`. To access gradients for intermediate nodes in PyTorch you can use hooks as mentioned in [this answer](https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94/2). Compute the values by hand (or check the slides) to verify your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 745,
     "status": "ok",
     "timestamp": 1581997998245,
     "user": {
      "displayName": "Samarth Mishra",
      "photoUrl": "",
      "userId": "00064519813488481618"
     },
     "user_tz": 300
    },
    "id": "4lj5N-fgvQ-v",
    "outputId": "296a7afc-268a-406d-b09e-61dd58b02c8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], grad_fn=<AddBackward0>) tensor([-12.], grad_fn=<MulBackward0>)\n",
      "tensor([-4.]) tensor([-4.]) tensor([3.]) None\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "q = x + y\n",
    "f = q*z\n",
    "print(q, f)\n",
    "f.backward(retain_graph=True)\n",
    "print(x.grad, y.grad, z.grad, q.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtBqrD7y3MeK"
   },
   "source": [
    "**1.1.3** If we now run `backward()` again, it will add the gradients to their previous values. Try it by running the above cell multiple times. This is useful in some cases, but if we just wanted to re-compute the gradients again, we need to zero them first, then run `backward()`. Add this step, then try running the  backward function multiple times to make sure the answer is the same each time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1581998000511,
     "user": {
      "displayName": "Samarth Mishra",
      "photoUrl": "",
      "userId": "00064519813488481618"
     },
     "user_tz": 300
    },
    "id": "g6qeSVAM0w1J",
    "outputId": "048e64e9-e6c1-4f28-ee51-099a2a401e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.]) tensor([-4.]) tensor([3.]) None\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "z.grad.zero_()\n",
    "f.backward(retain_graph=True)\n",
    "print(x.grad, y.grad, z.grad, q.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3lEIwkbv4qNO"
   },
   "source": [
    "### 1.2 Neuron\n",
    " Implement the function corresponding to one neuron (logistic regression unit) that we saw in the lecture and compute the gradient w.r.t. $x$ and $w$. The function is $f=\\sigma(w^Tx)$ where $\\sigma()$ is the sigmoid function. Initialize $x=[-1, -2, 1]$ and the weights to $w=[2, -3, -3]$ where $w_3$ is the bias. Print out the gradients and double check their values by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1581998002809,
     "user": {
      "displayName": "Samarth Mishra",
      "photoUrl": "",
      "userId": "00064519813488481618"
     },
     "user_tz": 300
    },
    "id": "H0UBPl5Ti15G",
    "outputId": "aad7da03-7ba8-4e65-a604-259ce8df8efd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x= tensor([-1., -2.,  1.], requires_grad=True) \n",
      "w= tensor([ 2., -3., -3.], requires_grad=True) \n",
      "f(x,w)= tensor(0.7311, grad_fn=<MulBackward0>)\n",
      "The gradient of f() w.r.t. x is tensor([ 0.3932, -0.5898, -0.5898])\n",
      "The gradient of f() w.r.t. w is tensor([-0.1966, -0.3932,  0.1966])\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "x = torch.tensor([-1., -2., 1.], requires_grad=True)\n",
    "w = torch.tensor([2., -3., -3.], requires_grad=True)\n",
    "f1 = torch.dot(x, w)\n",
    "f2 = -1 * f1\n",
    "f3 = torch.exp(f2)\n",
    "f = 1 / (1 + f3)\n",
    "print(\"\\nx=\", x, \"\\nw=\", w, \"\\nf(x,w)=\", f)\n",
    "\n",
    "f.backward()\n",
    "print(\"The gradient of f() w.r.t. x is\", x.grad)\n",
    "print(\"The gradient of f() w.r.t. w is\", w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KLEQdosNk6t9"
   },
   "source": [
    "### 1.3. torch.nn\n",
    " We will now implement the same neuron function $f$ with the same variable values as in Q1.2, but using the `Linear` class from `torch.nn`, followed by the [Sigmoid](https://pytorch.org/docs/stable/nn.html#torch.nn.Sigmoid) class. In general, many useful functions are already implemented for us in this package. Compute the gradients $\\partial f/\\partial w$ by running `backward()` and print them out (they will be stored in the Linear variable, e.g. in `.weight.grad`.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1581998005758,
     "user": {
      "displayName": "Samarth Mishra",
      "photoUrl": "",
      "userId": "00064519813488481618"
     },
     "user_tz": 300
    },
    "id": "zH3f8sj0ywQ3",
    "outputId": "d56c09a9-0add-40a7-da62-6cdf89ec8c63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights: Parameter containing:\n",
      "tensor([[ 2., -3.]], requires_grad=True)\n",
      "\n",
      "bias: Parameter containing:\n",
      "tensor([-3.], requires_grad=True)\n",
      "\n",
      "f: tensor([0.7311], grad_fn=<SigmoidBackward>)\n",
      "The gradient of f() w.r.t. w is tensor([[-0.1966, -0.3932]]) tensor([0.1966])\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "import torch.nn as nn\n",
    "\n",
    "linear_f = nn.Linear(2, 1)\n",
    "linear_f.weight.data = torch.tensor([[ 2., -3.]]);\n",
    "linear_f.bias.data = torch.tensor([ -3.]);\n",
    "\n",
    "print(\"\\nweights:\", linear_f.weight)\n",
    "print(\"\\nbias:\",linear_f.bias)\n",
    "\n",
    "sigmoid_f = nn.Sigmoid()\n",
    "\n",
    "input = torch.tensor([-1., -2.])\n",
    "\n",
    "f = sigmoid_f(linear_f(input))\n",
    "print(\"\\nf:\", f)\n",
    "\n",
    "# do backprop\n",
    "f.backward()\n",
    "\n",
    "print(\"The gradient of f() w.r.t. w is\", linear_f.weight.grad, linear_f.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isiTRoJuqnEL"
   },
   "source": [
    "### 1.4. Module\n",
    " Now lets put these two functions (Linear and Sigmoid) together into a \"module\". Read the [Neural Networks tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) if you have not already.\n",
    "\n",
    "**1.4.1** This code makes a subclass of the `Module` class, called `Neuron`, and sets variables to the same values as above. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8j_S7AQqQB8-"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Neuron(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Neuron, self).__init__()\n",
    "        # an affine operation: y = weight*x + bias, with fixed parameters\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.linear.weight.data = torch.tensor([[ 2., -3.]]);\n",
    "        self.linear.bias.data = torch.tensor([-3.]);\n",
    "        # a sigmoid function, elementwise\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Hv5iduNSLX9"
   },
   "source": [
    "**1.4.2** Now create a  variable of your `Neuron` class called `my_neuron` and run backpropagation on it. Print out the gradients again. Make sure you zero out the gradients first, by calling `.zero_grad()` function of the parent class. Even if you will not re-compute the backprop, it is good practice to do this every time to avoid accumulating gradient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1581998018634,
     "user": {
      "displayName": "Samarth Mishra",
      "photoUrl": "",
      "userId": "00064519813488481618"
     },
     "user_tz": 300
    },
    "id": "AG5V8LiVSQWn",
    "outputId": "adedcde3-bf02-4e9b-da26-b813d75336c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "The weights are: Parameter containing:\n",
      "tensor([[ 2., -3.]], requires_grad=True)\n",
      "\n",
      "f(x,w)= tensor([0.7311], grad_fn=<SigmoidBackward>)\n",
      "The gradient of f() w.r.t. w is tensor([[-0.1966, -0.3932]]) tensor([0.1966])\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "my_neuron = Neuron()\n",
    "print(my_neuron)\n",
    "params = list(my_neuron.parameters())\n",
    "\n",
    "print(\"The weights are:\", params[0])  # linear layer's .weight\n",
    "input = torch.tensor([-1., -2.])\n",
    "out = my_neuron(input)\n",
    "print(\"\\nf(x,w)=\", out)\n",
    "my_neuron.zero_grad()\n",
    "out.backward()\n",
    "print(\"The gradient of f() w.r.t. w is\", params[0].grad, params[1].grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5A5kaQxkLsFi"
   },
   "source": [
    "### 1.5. Loss and SGD\n",
    " Now, lets train our neuron on some data. The code below creates a toy dataset containing a few inputs $x$ and outputs $y$ (a binary 0/1 label), as well as a function that plots the data and current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1581998902715,
     "user": {
      "displayName": "Samarth Mishra",
      "photoUrl": "",
      "userId": "00064519813488481618"
     },
     "user_tz": 300
    },
    "id": "4nMCQG3-L94R",
    "outputId": "48318f4f-419c-4f54-ffff-a6065573cf3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 = 2.0 w1 = -3.0 bias = -3.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create some toy 2-D datapoints with binary (0/1) labels\n",
    "x = torch.tensor([[1.2, 1], [0.2, 1.4], [0.5, 0.5], \n",
    "                  [-1.5, -1.3], [0.2, -1.4], [-0.7, -0.5]])\n",
    "y = torch.tensor([0, 0, 0, 1, 1, 1 ])\n",
    "\n",
    "def plot_soln(x, y, params):\n",
    "  plt.plot(x[y==1,0], x[y==1,1], 'r+')\n",
    "  plt.plot(x[y==0,0], x[y==0,1], 'b.')\n",
    "  plt.grid(True)\n",
    "  plt.axis([-2, 2, -2, 2])\n",
    "  \n",
    "  # NOTE : This may depend on how you implement Neuron.\n",
    "  #   Change accordingly\n",
    "  w0 = params[0][0][0].item()\n",
    "  w1 = params[0][0][1].item()\n",
    "  bias = params[1][0].item()\n",
    "  \n",
    "  print(\"w0 =\", w0, \"w1 =\", w1, \"bias =\", bias)\n",
    "  dbx = torch.tensor([-2, 2])\n",
    "  dby = -(1/w1)*(w0*dbx + bias)  # plot the line corresponding to the weights and bias\n",
    "  plt.plot(dbx, dby)\n",
    "\n",
    "params = list(my_neuron.parameters())\n",
    "plot_soln(x, y, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CpB2AwwhckYc"
   },
   "source": [
    "**1.5.1** Declare an object `criterion` of type `nn.CrossEntropyLoss`. Note that this can be called as a function on two tensors, one representing the network outputs and the other, the targets that the network is being trained to predict, to return the loss. Print the value of the loss on the dataset using the initial weights and bias defined above in Q1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1581998912941,
     "user": {
      "displayName": "Samarth Mishra",
      "photoUrl": "",
      "userId": "00064519813488481618"
     },
     "user_tz": 300
    },
    "id": "l1M8bkH5cnVQ",
    "outputId": "8b79c853-db3e-4688-856e-1c7663195fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.9425851702690125\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# forward + backward + optimize\n",
    "outputs = my_neuron(x)\n",
    "loss = criterion(torch.cat((outputs, 1-outputs), axis=1), y)\n",
    "print(\"loss =\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJqHzmDL6yYh"
   },
   "source": [
    "**1.5.2** The following prints out the chain of `grad_fn` functions backwards starting from `loss.grad_fn`  to demonstrate what backpropagation will be run on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1581998915620,
     "user": {
      "displayName": "Samarth Mishra",
      "photoUrl": "",
      "userId": "00064519813488481618"
     },
     "user_tz": 300
    },
    "id": "wawoYnT3U04S",
    "outputId": "cfafafbe-8cd0-48bb-c6a4-bda00371a103",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NllLossBackward object at 0x7f7f34ef98d0>\n",
      "<LogSoftmaxBackward object at 0x7f7f34ef9898>\n",
      "<CatBackward object at 0x7f7f353eed30>\n",
      "<SigmoidBackward object at 0x7f7f34ef9898>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  \n",
    "print(loss.grad_fn.next_functions[0][0])  \n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  \n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Og0ZO46h3UNg"
   },
   "source": [
    "**1.5.3** Run the Stochastic Gradient Descent (SGD) optimizer from the `torch.optim` package to train your classifier on the toy dataset. Use the entire dataset in each batch. Use a learning rate of $0.01$ (no other hyperparameters). You will need to write a training loop that uses the `.step()` function of the optimizer. Plot the solution and print the loss after 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1098,
     "status": "ok",
     "timestamp": 1581998918323,
     "user": {
      "displayName": "Samarth Mishra",
      "photoUrl": "",
      "userId": "00064519813488481618"
     },
     "user_tz": 300
    },
    "id": "fqxdnM-vV1DU",
    "outputId": "3699a412-a9bb-48ad-d873-53349a233a23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.870725691318512\n",
      "w0 = 2.161842107772827 w1 = -2.233754873275757 bias = -3.3213229179382324\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# solution here\n",
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(my_neuron.parameters(), lr=0.01)\n",
    "\n",
    "# training loop\n",
    "for i in range(1000):\n",
    "  # in your training loop:\n",
    "  optimizer.zero_grad()   # zero the gradient buffers\n",
    "  outputs = my_neuron(x)\n",
    "  loss = criterion(torch.cat((outputs, 1-outputs), axis=1), y)\n",
    "  #print(\"loss =\", loss.item())\n",
    "  loss.backward()\n",
    "  optimizer.step()    # Does the update\n",
    "\n",
    "print(\"loss =\", loss.item())\n",
    "params = list(my_neuron.parameters())\n",
    "plot_soln(x, y, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0NBjzHF6TN5"
   },
   "source": [
    "**1.5.4** How many thousands of iterations does it take (approximately) until the neuron learns to classify the data correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wc4CEo9c6neD"
   },
   "source": [
    "Answer: approximately 5,000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xEV3Px886zrI"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Problem 2: Convolutional Networks (30 points)\n",
    "\n",
    "In this part, we will experiment with CNNs in PyTorch. You will need to read the documentation of the functions provided below to understand how they work.\n",
    "\n",
    "**GPU Training.** Smaller networks will train fine on a CPU, but you may want to use GPU training for this part of the homework. You can run your experiments on Colab's GPUs or on BU's  [Shared Computing Cluster (SCC)](http://www.bu.edu/tech/services/research/computation/scc/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kxl45fh6FNq6"
   },
   "source": [
    "### 2.1. Training a CNN on SVHN\n",
    "We will create and train a convolutional network on the [SVHN Dataset](http://ufldl.stanford.edu/housenumbers/).\n",
    "\n",
    "The SVHN dataset consists of photos of house numbers, collected automatically using Google's Street View. Recognizing multi-digit numbers in photographs captured at street level is an important component of modern-day map making. Googleâ€™s Street View imagery contains hundreds of millions of geo-located 360 degree panoramic images. The ability to automatically transcribe an address number from a geo-located patch of pixels and associate the transcribed number with a known street address helps pinpoint, with a high degree of accuracy, the location of the building it represents. Below are example images from the dataset. Note that for this dataset, each image (32x32 pixels) has been cropped around a single number in its center, which is the number we want to classify.\n",
    "\n",
    "![SVHN images](http://ufldl.stanford.edu/housenumbers/32x32eg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJa-vlG99lMr"
   },
   "source": [
    "### 2.1.2 Data Download\n",
    "The following downloads the SVHN dataset using `torchvision` and displays the images in the first batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHFWPxi6jCoB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.SVHN(root='./data', split='train',\n",
    "                                        transform=transform, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.SVHN(root='./data', split='test',\n",
    "                                        transform=transform, download=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Ob4Pvz-GuKA"
   },
   "source": [
    "### 2.1.3. CNN Model\n",
    "Next, we will train a CNN on the data. We have defined a simple CNN for you with two convolutional layers and two fully-connected layers below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hMXRfs8DPjx"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)   # flatten features\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z325ABykG77h"
   },
   "source": [
    "Instantiate the cross-entropy loss `criterion`, and an SGD optimizer from the `torch.optim` package with learning rate $.001$ and momentum $.9$. You may also want to enable GPU training using `torch.device()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wiSsTnEGCRN"
   },
   "outputs": [],
   "source": [
    "# solution here\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if we set the hardware to GPU in the Notebook settings, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCLHI077fl9x"
   },
   "source": [
    "### 2.1.4 Training\n",
    "Write the training loop that makes two full passes through the dataset (two epochs) using SGD. Your batch size should be 4.\n",
    "\n",
    "Go slack off for a while...\n",
    "\n",
    "![reddit xkcd comic](https://i.redd.it/5cjdqxcg07k11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u8SluNefGZgf"
   },
   "outputs": [],
   "source": [
    "# solution here\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TGkQdltRh104"
   },
   "source": [
    "### 2.1.5 Test Accuracy\n",
    "Load the test data (don't forget to move it to GPU if using). Make predictions on it using the trained network and compute the accuracy. You should see an accuracy of around 84%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zWHZOr2sHeB5"
   },
   "outputs": [],
   "source": [
    "# solution here\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images.cpu()))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "# make predictions\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6mCBCn8kX9W"
   },
   "source": [
    "# Problem 3: Tensorboard (Optional)\n",
    "\n",
    "Explore your network using Tensorboard. Tensorboard is a nice tool for visualizing how your network's training is progressing. The following tutorial provides an introduction to Tensorboard\n",
    "\n",
    "- [Visualizing models, data and training with Tensorboard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n",
    "\n",
    "For using tensorboard in colab, run the following cell and it should open a tensorboard interface in the output of the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILFfAGAyh8bD"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW3.ipynb",
   "provenance": [
    {
     "file_id": "1siLMvcwxUl6WSTO8JNjUv5T3TXMtujiH",
     "timestamp": 1582058789500
    },
    {
     "file_id": "1fiK0rbc8iMuQQS6PDijcD2MIS95h-oZe",
     "timestamp": 1581824994288
    },
    {
     "file_id": "https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/autograd_tutorial.ipynb",
     "timestamp": 1578596632955
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
